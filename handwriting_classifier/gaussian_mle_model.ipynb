{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as spio\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import time\n",
    "\n",
    "class GaussianProbabilisticModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_classes = 0\n",
    "        self.classes = []\n",
    "        self.mle_mu_map = {}\n",
    "        self.mle_sigma_map = {}\n",
    "        self.class_prob_map = {}\n",
    "        self.dim = 0\n",
    "        self.topN_features = 200\n",
    "        self.l = 0.1\n",
    "\n",
    "    def __log_gaussian_pdf(self,x,mu,sigma,sigma_new_I_given = None,logdet_given = None,compute_inv_and_det = True):\n",
    "        # calculate log of f(x)~multivariate Normal density\n",
    "        # sigma is adjusted by 0.1*I to avoid inverting a singular matrix\n",
    "        dim_of_cut_data = len(mu)\n",
    "        sigma_new = np.matrix(sigma+self.l*np.eye(dim_of_cut_data))\n",
    "        \n",
    "        # can feed the inverse and det in directly so that we don't have to compute it every time\n",
    "        if (compute_inv_and_det):\n",
    "            sigma_new_I = sigma_new.I\n",
    "            (sign, logdet) = np.linalg.slogdet(sigma_new)\n",
    "        else:\n",
    "            sigma_new_I = sigma_new_I_given\n",
    "            logdet = logdet_given\n",
    "        \n",
    "        C = -0.5*logdet + (-dim_of_cut_data*0.5)*np.log(2*math.pi)\n",
    "        x_minus_mu_mat = np.matrix(x-mu).T\n",
    "        result = C+float(-0.5*x_minus_mu_mat.T*sigma_new_I*x_minus_mu_mat)\n",
    "\n",
    "        return result\n",
    "                \n",
    "       \n",
    "    def train_model(self,training_data,labels):\n",
    "        # Steps:\n",
    "        # 1: Find the N=200 most significant features, and slice the data based on those features\n",
    "        # 2: Partition data according to labels\n",
    "        # 3: Compute MLE of mu and sigma for each label\n",
    "        \n",
    "        # Step 1.1 - find the MLE of mu and sigma for each feature over the entire dataset\n",
    "       \n",
    "        for i in range(len(training_data)):\n",
    "            if (i == 0):\n",
    "                feature_mle_mu = training_data[i]\n",
    "                self.dim = (training_data[i].shape)[0]\n",
    "            else:\n",
    "                feature_mle_mu += training_data[i]\n",
    "            \n",
    "        feature_mle_mu = feature_mle_mu * 1.0/len(training_data)\n",
    "        \n",
    "        for i in range(len(training_data)):\n",
    "            x_minus_mu_feature = training_data[i]-feature_mle_mu\n",
    "            if (i == 0):\n",
    "                feature_mle_sigma = x_minus_mu_feature**2\n",
    "            else:\n",
    "                feature_mle_sigma += x_minus_mu_feature**2\n",
    "                \n",
    "        feature_mle_sigma = feature_mle_sigma * 1.0/len(training_data)\n",
    "        \n",
    "        # Step 1.2 - find the N=200 most significant features (features exhibiting the largest variance)\n",
    "        keys = np.linspace(0,self.dim-1,self.dim)\n",
    "        feature_var = list(zip(keys,feature_mle_sigma))\n",
    "        \n",
    "        feature_var_sorted = sorted(feature_var, key=lambda f: f[1], reverse = True)\n",
    "        self.topN = [int(e[0]) for e in feature_var_sorted[:self.topN_features]]  \n",
    "        \n",
    "        # Step 1.3 - slice the training data on the most significant features\n",
    "        training_data = training_data[:,self.topN]\n",
    "        self.train_size = len(training_data)\n",
    "        nums={}\n",
    "        \n",
    "        # Step 2.1 - calculate MLE of mu for each label\n",
    "        for i in range(len(training_data)):\n",
    "            if not (labels[i] in self.mle_mu_map):\n",
    "                self.mle_mu_map[labels[i]] = training_data[i]\n",
    "                self.classes += [labels[i]]\n",
    "                nums[labels[i]] = 1\n",
    "                self.num_classes += 1\n",
    "                \n",
    "            else:\n",
    "                self.mle_mu_map[labels[i]] += training_data[i]\n",
    "                nums[labels[i]] += 1\n",
    "                \n",
    "        for i in self.classes:\n",
    "            self.mle_mu_map[i]=(1.0/nums[i])*self.mle_mu_map[i]\n",
    "        \n",
    "        # Step 2.2 - calculate MLE of sigma for each label\n",
    "        for i in range(len(training_data)):\n",
    "            # this is a column vector\n",
    "            x_minus_mu = np.matrix(training_data[i]-self.mle_mu_map[labels[i]]).T\n",
    "            \n",
    "            if not (labels[i] in self.mle_sigma_map):\n",
    "                self.mle_sigma_map[labels[i]]=(x_minus_mu)*(x_minus_mu.T)\n",
    "            else:\n",
    "                self.mle_sigma_map[labels[i]]+=(x_minus_mu)*(x_minus_mu.T)\n",
    "               \n",
    "        for i in self.classes:\n",
    "            self.mle_sigma_map[i]=(1.0/nums[i])*self.mle_sigma_map[i]\n",
    "            self.class_prob_map[i]=(nums[i]*1.0)/len(training_data)\n",
    "         \n",
    "        # precalculate matrix inverses and determinants to use in prediction function\n",
    "        # the \"new\" refers to computing the inverse of sigma + l*I \n",
    "        self.sigma_new_inverses = {}\n",
    "        self.logdets = {}\n",
    "        for i in self.classes:\n",
    "            self.sigma_new_inverses[i]=np.matrix(self.mle_sigma_map[i] + self.l*np.eye(self.topN_features)).I\n",
    "            self.logdets[i]=np.linalg.slogdet(np.matrix(self.mle_sigma_map[i] + self.l*np.eye(self.topN_features)))[1]\n",
    "            \n",
    "    def predict(self,x):\n",
    "        # prediction is equal to the maximum likelihood class\n",
    "        prediction=-1\n",
    "        curr_max=-1e300;\n",
    "        \n",
    "        for i in self.classes:\n",
    "            p = self.__log_gaussian_pdf(x,self.mle_mu_map[i],self.mle_sigma_map[i],\n",
    "                                       sigma_new_I_given = self.sigma_new_inverses[i],\n",
    "                                       logdet_given = self.logdets[i],\n",
    "                                       compute_inv_and_det = False) + np.log(self.class_prob_map[i])\n",
    "            if(p>curr_max):\n",
    "                curr_max=p\n",
    "                prediction=i\n",
    "        return prediction\n",
    "                        \n",
    "    def test_model(self,test_data,labels):\n",
    "        misses=0        \n",
    "        # slice the data based on the most significant features\n",
    "        test_data = test_data[:,self.topN]\n",
    "        self.test_size = len(test_data)\n",
    "        results = {'Test Error': [], 'Training Size': [], 'Testing Size': [], 'Predicted': [], 'Actual': []}\n",
    "        results['Training Size'] = self.train_size\n",
    "        results['Testing Size'] = self.test_size\n",
    "\n",
    "        for i in range(len(test_data)):\n",
    "            pred = self.predict(test_data[i])\n",
    "            actual = labels[i]\n",
    "            \n",
    "            results['Predicted'] += [pred]\n",
    "            results['Actual'] += [actual]\n",
    "            \n",
    "            if(pred != actual):\n",
    "                misses=misses+1\n",
    "                \n",
    "        results['Test Error'] = misses*1.0/len(test_data)\n",
    "        return results\n",
    "\n",
    "\n",
    "#mat = spio.loadmat('hw1data.mat', squeeze_me=True)\n",
    "#image_matrix=mat['X']\n",
    "#label_array=mat['Y']   \n",
    "\n",
    "#tstart = time.time()\n",
    "    \n",
    "#gaussian_model= GaussianProbabilisticModel()\n",
    "#gaussian_model.train_model(image_matrix[:3000,:],label_array[:3000])\n",
    "#tmid = time.time()\n",
    "#print('Training complete')\n",
    "#print(gaussian_model.test_model(image_matrix[3001:3301,:],label_array[3001:3301]))\n",
    "\n",
    "#tend = time.time()\n",
    "#print(\"Training time: \" + str(tmid-tstart))\n",
    "#print(\"Testing time: \" + str(tend-tmid))\n",
    "#print(\"Total time taken: \" + str(tend-tstart))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kNN Classifier\n",
    "\n",
    "def EuclideanDistance(x,y):\n",
    "    return np.linalg.norm(np.array(y)-np.array(x))\n",
    "\n",
    "def L1Distance(x,y):\n",
    "    return np.sum(abs(np.array(y)-np.array(x)))\n",
    "\n",
    "def LinfDistance(x,y):\n",
    "    return max(abs(np.array(y)-np.array(x)))\n",
    "\n",
    "class kNNModel:\n",
    "    \n",
    "    def __init__(self, dist_function):\n",
    "        self.dist_function = dist_function\n",
    "        self.topN_features = 200\n",
    "        \n",
    "    # computes the distance from x to each element in lst, which is a list of vectors\n",
    "    def __dist(self, x, lst, lst_labels):\n",
    "        dists = [self.dist_function(x,e) for e in lst]\n",
    "        vec_label_dist_tuple = list(zip(lst,lst_labels,dists))\n",
    "        return vec_label_dist_tuple\n",
    "                \n",
    "    def train_model(self,training_data,labels):\n",
    "        # k will be log(# of data points) rounded to the nearest integer\n",
    "        self.k = int(np.log(len(training_data)) + 0.5)\n",
    "        \n",
    "        # Step 1.1 - find the MLE of mu and sigma for each feature over the entire dataset\n",
    "       \n",
    "        for i in range(len(training_data)):\n",
    "            if (i == 0):\n",
    "                feature_mle_mu = training_data[i]\n",
    "                self.dim = (training_data[i].shape)[0]\n",
    "            else:\n",
    "                feature_mle_mu += training_data[i]\n",
    "            \n",
    "        feature_mle_mu = feature_mle_mu * 1.0/len(training_data)\n",
    "        \n",
    "        for i in range(len(training_data)):\n",
    "            x_minus_mu_feature = training_data[i]-feature_mle_mu\n",
    "            if (i == 0):\n",
    "                feature_mle_sigma = x_minus_mu_feature**2\n",
    "            else:\n",
    "                feature_mle_sigma += x_minus_mu_feature**2\n",
    "                \n",
    "        feature_mle_sigma = feature_mle_sigma * 1.0/len(training_data)\n",
    "        \n",
    "        # Step 1.2 - find the N=200 most significant features (features exhibiting the largest variance)\n",
    "        keys = np.linspace(0,self.dim-1,self.dim)\n",
    "        feature_var = list(zip(keys,feature_mle_sigma))\n",
    "        \n",
    "        feature_var_sorted = sorted(feature_var, key=lambda f: f[1], reverse = True)\n",
    "        self.topN = [int(e[0]) for e in feature_var_sorted[:self.topN_features]]  \n",
    "        \n",
    "        # Step 1.3 - slice the training data on the most significant features\n",
    "        training_data = training_data[:,self.topN]\n",
    "        \n",
    "        self.labels = labels\n",
    "        self.data = training_data.tolist()\n",
    "        self.train_size = len(training_data)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        vec_label_dist_tuple = self.__dist(x,self.data,self.labels)\n",
    "        # sort by distance\n",
    "        vec_label_dist_tuple_sorted = sorted(vec_label_dist_tuple, key=lambda f: f[2])\n",
    "        # get an array containing (label,distance) tuples\n",
    "        k_closest = vec_label_dist_tuple_sorted[:self.k]\n",
    "        # group by frequency of label\n",
    "        count_dic = {}\n",
    "        for tup in k_closest:\n",
    "            if tup[1] not in count_dic:\n",
    "                count_dic[tup[1]] = 1.0\n",
    "            else:\n",
    "                count_dic[tup[1]] += 1.0\n",
    "        \n",
    "        prediction = max(count_dic, key=count_dic.get)\n",
    "        return prediction          \n",
    "        \n",
    "    def test_model(self,test_data,labels):\n",
    "        misses=0        \n",
    "        # slice the data based on the most significant features\n",
    "        test_data = test_data[:,self.topN]\n",
    "        self.test_size = len(test_data)\n",
    "        results = {'Test Error': [], 'Training Size': [], 'Testing Size': [], 'Predicted': [], 'Actual': []}\n",
    "        results['Training Size'] = self.train_size\n",
    "        results['Testing Size'] = self.test_size\n",
    "        \n",
    "        for i in range(len(test_data)):\n",
    "            pred = self.predict(test_data[i])\n",
    "            actual = labels[i]\n",
    "            \n",
    "            results['Predicted'] += [pred]\n",
    "            results['Actual'] += [actual]\n",
    "            \n",
    "            if(pred != actual):\n",
    "                misses += 1\n",
    "                \n",
    "        results['Test Error'] = misses*1.0/len(test_data)\n",
    "        return results\n",
    "    \n",
    "mat = spio.loadmat('hw1data.mat', squeeze_me=True)\n",
    "image_matrix=mat['X']\n",
    "label_array=mat['Y']   \n",
    "\n",
    "tstart = time.time()\n",
    "#kNN_model_L2 = kNNModel(EuclideanDistance)\n",
    "#kNN_model_L2.train_model(image_matrix[:1000,:],label_array[:1000])\n",
    "#print(kNN_model_L2.test_model(image_matrix[1001:1101,:],label_array[1001:1101]))\n",
    "#print(\"L2 Test error: \" + str(L2_test_error))\n",
    "#tend = time.time()\n",
    "#print(\"Total time taken: \" + str(tend-tstart))\n",
    "\n",
    "#kNN_model_L1 = kNNModel(L1Distance)\n",
    "#kNN_model_L1.train_model(image_matrix[:1000,:],label_array[:1000])\n",
    "#print(kNN_model_L1.test_model(image_matrix[1001:1101,:],label_array[1001:1101]))\n",
    "#print(\"L1 Test error: \" + str(L1_test_error))\n",
    "\n",
    "#kNN_model_Linf = kNNModel(LinfDistance)\n",
    "#kNN_model_Linf.train_model(image_matrix[:1000,:],label_array[:1000])\n",
    "#print(kNN_model_Linf.test_model(image_matrix[1001:1101,:],label_array[1001:1101]))\n",
    "#print(\"Linf Test error: \" + str(Linf_test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def shuffle_and_split(data,partition):\n",
    "    ran_order = np.arange(len(data['X']))\n",
    "    np.random.shuffle(ran_order)\n",
    "    ran_order_training = ran_order[:(int)(len(data['X'])*partition)] \n",
    "    ran_order_test = ran_order[(int)(len(data['X'])*partition):] \n",
    "    training_data = data['X'][ran_order_training]\n",
    "    training_label = data['Y'][ran_order_training]\n",
    "    test_data = data['X'][ran_order_test]\n",
    "    test_label = data['Y'][ran_order_test]\n",
    "    return [training_data,training_label,test_data,test_label]\n",
    "\n",
    "\n",
    "mat = spio.loadmat('hw1data.mat', squeeze_me=True)\n",
    "mat['X']=mat['X'][0:2000,:]\n",
    "mat['Y']=mat['Y'][0:2000]\n",
    "K=[]\n",
    "knn_training_error=[]\n",
    "knn_test_error=[]\n",
    "gaussian_training_error=[]\n",
    "gaussian_test_error=[]\n",
    "gaussian_model= GaussianProbabilisticModel()\n",
    "kNN_model_L2 = kNNModel(EuclideanDistance)\n",
    "for k in range(40,100,10):\n",
    "    gaussian_model= GaussianProbabilisticModel()\n",
    "    kNN_model_L2 = kNNModel(EuclideanDistance)\n",
    "    K.append(k/100)\n",
    "    [training_data,training_label,test_data,test_label]=shuffle_and_split(mat,k/100)\n",
    "    kNN_model_L2.train_model(training_data,training_label)\n",
    "    gaussian_model.train_model(training_data,training_label)\n",
    "    knn_training_error.append(kNN_model_L2.test_model(training_data,training_label))\n",
    "    knn_test_error.append(kNN_model_L2.test_model(test_data,test_label))\n",
    "    gaussian_training_error.append(gaussian_model.test_model(training_data,training_label))\n",
    "    gaussian_test_error.append(gaussian_model.test_model(test_data,test_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-a1ed95e0b5d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mknn_training_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Test Error'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mknn_training_error\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mknn_test_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Test Error'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mknn_test_error\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgaussian_training_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Test Error'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgaussian_training_error\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-a1ed95e0b5d1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mknn_training_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Test Error'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mknn_training_error\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mknn_test_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Test Error'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mknn_test_error\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgaussian_training_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Test Error'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgaussian_training_error\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "K=[40,50,60,70,80,90]\n",
    "knn_training_error=[e['Test Error'] for e in knn_training_error ]\n",
    "knn_test_error=[e['Test Error'] for e in knn_test_error ]\n",
    "gaussian_training_error=[e['Test Error'] for e in gaussian_training_error ]\n",
    "gaussian_test_error=[e['Test Error'] for e in gaussian_test_error ]\n",
    "print(knn_training_error)\n",
    "plt.plot(K,knn_training_error,label=\"kNN-Training Error\")\n",
    "plt.plot(K,knn_test_error,label=\"kNN-Testing Error\")\n",
    "plt.plot(K,gaussian_training_error,label=\"kNN-Training Error\")\n",
    "plt.plot(K,gaussian_test_error,label=\"kNN-Testing Error\")\n",
    "plt.xlabel(\"Model complexity->\")\n",
    "plt.ylabel(\"Error->\")\n",
    "plt.gca().set_ylim([0,1])\n",
    "lt.gcf().set_size_inches(18,8)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[training_data,training_label,test_data,test_label]=shuffle_and_split(mat,0.8)\n",
    "l2_test_error=[]\n",
    "l1_test_error=[]\n",
    "linf_test_error=[]\n",
    "K=[]\n",
    "for k in range(1,10):\n",
    "    K.append(k)\n",
    "    kNN_model_L2 = kNNModel(EuclideanDistance)\n",
    "    kNN_model_L1 = kNNModel(L1Distance)\n",
    "    kNN_model_Linf = kNNModel(LinfDistance)\n",
    "    kNN_model_L2.train_model(training_data,training_label)\n",
    "    kNN_model_L1.train_model(training_data,training_label)\n",
    "    kNN_model_Linf.train_model(training_data,training_label)\n",
    "    l2_test_error.append(kNN_model_L2.test_model(test_data,test_label))\n",
    "    l1_test_error.append(kNN_model_L1.test_model(test_data,test_label))\n",
    "    linf_test_error.append(kNN_model_L2.kNN_model_Linf(test_data,test_label))\n",
    "\n",
    "l2_test_error=[e['Test Error'] for e in l2_test_error ]\n",
    "l1_test_error=[e['Test Error'] for e in l1_test_error ]\n",
    "linf_test_error=[e['Test Error'] for e in linf_test_error ]\n",
    "plt.plot(K,l2_test_error,label=\"kNN-L2\")\n",
    "plt.plot(K,l1_test_error,label=\"kNN-L1\")\n",
    "plt.plot(K,linf_test_error,label=\"kNN-Linf\")\n",
    "plt.xlabel(\"K(nearest neighbours)->\")\n",
    "plt.ylabel(\"Error->\")\n",
    "plt.gca().set_ylim([0,1])\n",
    "plt.gca().set_xlim([0,1])\n",
    "plt.gcf().set_size_inches(18,8)\n",
    "plt.title(\"Training Error for different distance metrics->\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
